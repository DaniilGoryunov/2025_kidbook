# Отчет по лабораторной работе: KidBook - Травля (Буллинг)

## Состав команды 1

| ФИО                                | Роль и что делал                                                                                                | Оценка |
|------------------------------------|-----------------------------------------------------------------------------------------------------------------|--------|
| Андриянов Эрик Вячеславович М80-312Б-22 | Исследование Wikidata (SPARQL, визуализация), разработка скрипта для генерации текстов (GigaChat API + LangChain), написание отчета |        |
| Драновский Иван Петрович М80-312Б-22   | Концептуализация, построение онтологии (Markmap), разработка промптов, разработка скрипта для кросс-линковки (`pymorphy3`), написание отчета |        |
|                                               |        |

*Примечание: Оценки проставляются преподавателем.*

## Концептуализация предметной области

**Тема:** Здоровье -> Эмоции и стресс -> Травля (Буллинг)

**Процесс:**

1.  **Исследование источников:** Первым делом мы погрузились в существующие базы знаний, в первую очередь **Wikidata**. С помощью **SPARQL-запросов** и инструментов визуализации графов знаний (подобных [Metaphacts](https://metaphacts.com/) или [Wikidata Query Service](https://query.wikidata.org/) с опцией графа) мы изучили существующие понятия, связанные с "травлей" (Bullying, Q18579). Мы выявили ключевые подклассы (интернет-травля, травля в школе и т.д.) и связанные сущности (моббинг, агрессор, свидетель). Пример визуализации графа представлен ниже (скриншот из инструмента типа Metaphacts/Wikidata Visualization).

    ![Скриншот графа Wikidata для "Травля"](/WORK/health/emotions_stress/image.png)
2.  **Построение онтологии:** На основе изученных данных и требований (не менее 15 понятий) мы разработали концептуализацию нашей предметной области. Мы определили основные классы ("Травля", "Вид Травли", "Участник Травли", "Действие/Реакция") и связи между ними. Помимо иерархических связей (`является подклассом`), мы старались определить и горизонтальные связи, такие как `имеет участника`, `проявляется как`, `может привести к`, `противодействие`.

3.  **Визуализация:** Для наглядного представления онтологии мы использовали **Markmap.js**. Это позволило легко создать интерактивную ментальную карту из Markdown-разметки, включив в нее ID из Wikidata для ключевых понятий.

    ```markmap
    # Карта понятий: Травля (Буллинг)

    ## Травля (Буллинг) [Q18579]

    - **Виды травли** (`является подклассом`)
      - Травля в школе [Q1093944]
      - Интернет-травля (Кибербуллинг) [Q18583]
      - Физический буллинг [Q1856261]
      - Словесный буллинг [Q7920890]
      - Социальный буллинг <!-- ID сложнее найти -->

    - **Участники** (`имеет участника`)
      - Агрессор (Обидчик) [Q1934304]
      - Жертва (Пострадавший)
      - Свидетель [Q18570]

    - **Аспекты и отличия** (`имеет аспект`, `отличается от`)
      - Чувства жертвы
      - Отличие буллинга от ссоры

    - **Что делать? / Помощь** (`противодействие`, `рекомендация`)
      - Что делать, если обижают тебя?
      - Что делать, если видишь буллинг? (Роль свидетеля)
      - Кому рассказать о буллинге? (Взрослые помощники)
      - Как остановить буллинг? (Дружба и Уважение)

    - **Связанные понятия**
      - Моббинг [Q205384]
    ```

4.  **Список концептов:** Финальный список из 15 концептов был сохранен в файле `WORK/health/emotions_stress/concepts.json` в требуемом формате.

## Написание текстов

1.  **Выбор LLM:** Мы остановили свой выбор на **GigaChat**. Основные причины:
    *   **Доступность и лимиты:** GigaChat предоставлял достаточно щедрый бесплатный лимит на момент выполнения работы, что позволило нам экспериментировать с API без значительных затрат. "Халява, сэр!" - как говорится, отличный стимул для студенческих проектов.
    *   **Качество русского языка:** Модель хорошо справляется с генерацией текстов на русском языке, что было критично для нашей задачи.
    *   **Интеграция с LangChain:** Наличие готовой интеграции в популярной библиотеке `langchain-community` упростило программное взаимодействие с API.

2.  **Генерация через API:** Для повышения оценки и автоматизации процесса мы использовали **Python** и библиотеку **LangChain** для программного вызова GigaChat API. Был написан скрипт (`text_generator.py`), который:
    *   Читал список тем из `concepts.json`.
    *   Инициализировал клиент GigaChat через LangChain, используя ключ авторизации, вводимый безопасно через `getpass`.
    *   Использовал `PromptTemplate` и `LLMChain` для формирования запросов.
    *   Итерировал по списку тем, отправляя запросы к API с паузами (`time.sleep`), чтобы не превышать лимиты.
    *   Сохранял полученные тексты в соответствующие `.md` файлы в директории `KIDBOOK`, добавляя заголовок первого уровня.

3.  **Разработка промпта:** Ключевым моментом была разработка эффективного промпта. Мы использовали следующий шаблон:

    ```text
    Объясни для десятилетнего ребенка простыми словами, что такое "{topic}".

    Твой ответ будет использоваться для детской энциклопедии KidBook.
    Пожалуйста, оформи текст в формате Markdown. Используй:
    *   **Заголовки второго уровня (##)** для выделения важных подтем, если это уместно.
    *   **Списки (* или -)** для перечислений или шагов.
    *   **Выделение жирным шрифтом (**слово**)** для ключевых терминов или важных моментов.
    *   **Выделение курсивом (*слово*)** для определений или примеров.

    Сделай текст понятным, интересным и хорошо структурированным. Не используй заголовок первого уровня (#), так как он будет добавлен автоматически из названия темы.
    ```
    Этот промпт явно указывал на целевую аудиторию (10 лет), требуемый формат (Markdown) и давал конкретные рекомендации по структурированию.

4.  **Расстановка кросс-ссылок:** Для автоматической расстановки ссылок между статьями был разработан отдельный скрипт (`link_generator.py`).
    *   **Сбор данных:** Скрипт агрегировал информацию из всех `concepts.json` файлов в директории `WORK`.
    *   **Лемматизация:** Для учета различных форм слов (падежей, чисел) мы использовали библиотеку **`pymorphy3`**. Скрипт разделял концепты на однословные (для них применялась лемматизация) и многословные/сложные (для них использовался поиск по точному совпадению фразы).
    *   **Поиск и замена:** Скрипт обрабатывал каждый `.md` файл:
        *   Маскировал существующие Markdown-ссылки и блоки кода, чтобы избежать их повреждения.
        *   Сначала искал и заменял многословные/сложные термины (от длинных к коротким).
        *   Затем токенизировал оставшийся текст и искал однословные термины по их леммам, создавая ссылки с использованием *оригинального* слова из текста.
        *   Рассчитывал корректные **относительные пути** между файлами с помощью `os.path.relpath`.
        *   Снимал маскировку и перезаписывал файл (при наличии изменений и если не был включен режим `--dry-run`).
    *   **Обработка падежей:** Использование `pymorphy3` позволило частично решить проблему падежей для однословных терминов (например, найти "свидетеля", если тема "Свидетель").

## Выводы

**Успехи:**

*   Удалось успешно интегрировать данные из **Wikidata** в процесс концептуализации.
*   Создана онтология предметной области, включающая более 15 понятий и разные типы связей, визуализированная с помощью **Markmap**.
*   Реализована **автоматическая генерация текстов** для всех понятий с использованием **GigaChat API** и библиотеки **LangChain**.
*   Разработан скрипт для **автоматической расстановки кросс-ссылок**, использующий **лемматизацию (`pymorphy3`)** для повышения точности связывания в русском языке.

**Сложности и решения:**

1.  **Точность ссылок и падежи:** Несмотря на использование `pymorphy3`, автоматическая расстановка ссылок не идеальна. Некоторые формы слов могли быть пропущены, или, наоборот, могли быть созданы некорректные ссылки (например, слово используется в другом контексте).
    *   *Решение:* Комбинированный подход (лемматизация + точное совпадение), сортировка тем по длине, проверка на самоссылки. Требуется **ручной контроль** результатов.
2.  **Маскировка Markdown:** Обеспечить 100% надежную маскировку всех возможных конструкций Markdown (особенно вложенных) при замене текста – нетривиальная задача.
    *   *Решение:* Использование регулярных выражений для основных конструкций (ссылки, код). Возможны ошибки на сложных случаях.
3.  **Качество LLM-генерации:** Хотя GigaChat показал хорошие результаты, иногда тексты требовали ручной доработки для улучшения стиля, фактологической точности или соответствия возрасту. Форматирование Markdown не всегда было идеальным.
    *   *Решение:* Тщательный подбор промпта, готовность к **постредактированию**.
4.  **Поиск в Wikidata:** Найти подходящие и точные QID для всех концептов (особенно для абстрактных или составных, вроде "Чувства жертвы") было сложно.
    *   *Решение:* Фокусировка на ключевых понятиях, использование поиска по синонимам на разных языках, принятие того, что не для всего есть прямой аналог.

**Возможные улучшения:**

*   **Улучшение скрипта линковки:** Использовать более продвинутые NLP-методы для идентификации мест для ссылок (не только по ключевым словам/леммам, но и по семантической близости). Внедрить более надежный парсер Markdown.
*   **Интеграция изображений:** Добавить автоматический поиск (например, через API поисковиков или специализированные сервисы) и вставку релевантных изображений в статьи.
*   **Более глубокая интеграция с Wikidata:** Использовать SPARQL-запросы не только для концептуализации, но и для извлечения фактов (например, дат, определений, связей), которые можно было бы включать в промпты для LLM для более фактологически точной генерации.
*   **Интерактивность:** Добавить тесты или викторины к статьям.

В целом, работа позволила на практике познакомиться с методами представления знаний, возможностями современных LLM и задачами автоматической обработки текста, показав как сильные стороны этих инструментов, так и существующие ограничения.

=======

## Состав команды 2

| ФИО         | Что делал           | Оценка |
|-------------|----------------|--------|
| Болгаев А.А.         | Генерация текстов, составление SPARQL запросов |      |
| Даутов Т.Б.         | Генерация текстов, составление графов | |
| Каримов А.А.         | Генерация текстов, генерация картинок |  |
| Сектименко И.В.        | Генерация текстов, составление связей, составление SPARQL запросов для группировки понятий | |
| Волков А.Д.        | Генерация текстов, обработка SPARQL запросов (фильтрация результатов), отчет | |

## Концептуализация предметной области

Для начала, мы зашли на wikidata, чтобы посмотреть какие есть темы. Затем с помощью SPARQL-запроса мы смогли составить граф с темами, которые связаны с эмоциями.

![Граф с wikidata](static/graph1.jpg)

SPARQL-запросы мы делали с помощью языка програмиирования `Python` с помощью библиотеки `SPARQLWrapper`.

SPARQL-запросом мы достали из wikidata подклассы (и их подклассы) эмоций. Посмотрев на получившийся граф, было принято решение его немного отфильтровать, а точнее взять все подклассы эмоций, которые содержат слово "эмоции". Так было получено деление эмоций на позитивные и негативные и примеры каждого класса эмоций.

Стресс - оказался частью негативных эмоций, что вполне логично. Из wikidata были получены причины стресса. Так же от себя были добавлены способы борьбы со стрессом.

По итогу, после того, как мы отфильтровали темы и разбили их на категории, мы приступуили к построению графа онтологии. Чтобы построить такой граф мы использовали инструмент `Markmap.js`. По итогу получился вот такой граф онтологии:

![Онтология](static/graph2.jpg)

После построения графа онтологии мы занялись генерацией текстов.

## Написание текстов

Для генерации текстов мы написали программу на `Python`, которая обращается к API сервиса duck.ai, который в свою очередь обращается к модели `GPT4-o-mini`. Для того, чтобы сгенерировать текст для всех понятий, мы запустили цикл по всем понятиям из `concepts.json` и обращались к API сервиса duck.ai.

Чтобы удовлетворять условиям лабораторной работы, в запросах мы использовали промпт: `Объясни для десятилетнего ребенка понятие <понятие>`. Таким образом тексты были написаны простым и понятным языком. Также, мы использовали еще один промпт, для того чтобы GPT сам связывал понятия, которые мы ему даем: `если понятие имеет связь с другими, то сделай на них ссылку в формате гиперссылки markdown [понятие](понятие.md)`. Из-за этого нам не надо было вручную придумывать и расставлять ссылки в текстах, достаточно было лишь исправить некоторые из-них, чтобы переходы по ссылкам срабатывали конкретно.

По итогу, `GPT` связала понятия подобным образом:

![Связи](static/graph3.jpg)

Как видно из графа `Obsidian`, темы четко разделились на категории негативных эмоций и позитивных эмоций. Также видно группу тем связанных со стрессом. Если посмотреть на наш граф онтологии, то так и должно было получиться.

Также мы использовали еще несколько промптов, для того чтобы модель писала текст по шаблону и добавляла туда соответствующиие эмодзи, чтобы добавить красочности в нашу энциклопедию. Итоговый промпт выглядит подобным образом:
```
Вот тебе список всех понятий {concepts}, связанных со стрессом. 
    Объясни максимально подробно для десятилетнего ребенка что такое {concept} на русском языке. 
    Приведи примеры ситуаций, когда десятилетний ребенок может испытать данную эмоцию.
    Пиши текст с такой структурой:
    - определение
    - примеры
    - способы решения
    - заключение
    Оформи каждый пункт выдели и добавь соответсвующие эмодзи.
    По возможности для объяснения понятия ссылайся на другие понятия из списка.

    Расставь ссылки на другие понятия из списка в формате гиперссылки markdown [понятие](понятие.md).
```
Также мы решили для каждого понятия сгенерировать по картинке с помощью программы на `Python`, которая обращается к API модели `GigaChat`. Аналогично генерации текстов, мы сгенерировали картинки для каждого понятия. Затем написали скрипт, который будет вставлять картинки в соответствующие markdown-файлы. По итогу получилась красочная детская энциклопедия.

## Выводы

Самая сложная работа заключалась в построении графа онтологии и в фильтрации результатов SPARQL-запросов. С графом онтологии получилось разобраться с помощью `Markmap.js`, трудности возникли с тем, что понятий оказалось слишком много и приходилось сужать граф, чтобы было все видно. Также возниколи трудности с генерацией картинок, приходилось делать несколько попыток для получения желаемого результата.

В будущем, можно улучшить навигацию, так как тем слишком много, сейчас сложно по ним перемещаться в поисках какой-то конкретной темы. Для этого можно ввести еще несколько категорий.

Нам понравилась наша энциклопедия, так как она получилась красочной и интересной. Детям будет интересно и увлекательно ее читать и каждый день узнавать что-то новое. 
